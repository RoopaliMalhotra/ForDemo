{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKLlqnW7Lg5gw657du6c0F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoopaliMalhotra/ForDemo/blob/main/Smart_Taxi_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gy9gQ34MtQz",
        "outputId": "542dfed7-0423-43c5-8b3b-93e987ed56bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Taxi-v3\", render_mode=\"ansi\", new_step_api=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "arWO9CBrNs1g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Q-table banate hain: rows = states, columns = actions\n",
        "state_size = env.observation_space.n\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Sab Q-values pehle 0 se fill kar dete hain\n",
        "q_table = np.zeros((state_size, action_size))\n",
        "\n",
        "print(f\"State space size: {state_size}\")\n",
        "print(f\"Action space size: {action_size}\")\n",
        "print(\"Q-table initialized successfully âœ…\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlifXiupNtiW",
        "outputId": "589d55b2-a778-4bd8-dbb1-c958eda5f952"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State space size: 500\n",
            "Action space size: 6\n",
            "Q-table initialized successfully âœ…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate (alpha): Kitna naya seekhna hai\n",
        "alpha = 0.1\n",
        "\n",
        "# Discount factor (gamma): Future reward ki importance\n",
        "gamma = 0.6\n",
        "\n",
        "# Exploration rate (epsilon): Random try karne ki chance\n",
        "epsilon = 0.1\n",
        "\n",
        "# Episodes: Kitne baar agent training karega\n",
        "episodes = 100000\n",
        "\n",
        "# Stats store karne ke liye lists\n",
        "all_epochs = []\n",
        "all_penalties = []\n"
      ],
      "metadata": {
        "id": "XMAXY-VsOFi-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Training loop for 'episodes' times\n",
        "for i in range(1, episodes + 1):\n",
        "    # Correct unpacking of the step function output\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    # Done is now a combination of 'terminated' and 'truncated'\n",
        "    done = terminated or truncated\n",
        "\n",
        "\n",
        "\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "\n",
        "    while not done:\n",
        "        # Exploration-exploitation tradeoff\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()  # Explore: random action\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])  # Exploit: best action from Q-table\n",
        "\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "        # Reward shaping (bonus for correct drop)\n",
        "        if reward == 20:\n",
        "            reward = 25\n",
        "\n",
        "        # Q-learning update rule\n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "        new_value = old_value + alpha * (reward + gamma * next_max - old_value)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "\n",
        "    all_epochs.append(epochs)\n",
        "    all_penalties.append(penalties)\n",
        "\n",
        "    # Clear output for better visual experience (optional)\n",
        "    if i % 10000 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "print(\"\\nTraining finished! ðŸŽ‰\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU2fbcb4OM75",
        "outputId": "060b8bd7-f0cb-4ab6-e5b3-26cb312af26d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100000\n",
            "\n",
            "Training finished! ðŸŽ‰\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UDTaDPyPONlB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation - test trained agent\n",
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100  # Set episodes for evaluation\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()  # Now returns just the state\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    done = False\n",
        "    max_steps = 1000\n",
        "    steps = 0\n",
        "\n",
        "    while not done and steps < max_steps:\n",
        "        action = np.argmax(q_table[state])  # Best action from Q-table\n",
        "        step_result = env.step(action)  # Get the result of the action\n",
        "\n",
        "        # Unpack the first 3 values (next_state, reward, done)\n",
        "        next_state, reward, done, _, _ = step_result\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "        steps += 1\n",
        "\n",
        "    total_epochs += epochs\n",
        "    total_penalties += penalties\n",
        "\n",
        "    # Optional: Print progress every 10 episodes\n",
        "    if episode % 10 == 0:\n",
        "        print(f\"Episode {episode} completed.\")\n",
        "\n",
        "# Calculate average metrics\n",
        "average_penalties = total_penalties / episodes\n",
        "average_epochs = total_epochs / episodes\n",
        "\n",
        "print(\"\\nâœ… Evaluation finished!\")\n",
        "print(f\"ðŸ“‰ Average penalties per episode: {average_penalties}\")\n",
        "print(f\"â±ï¸ Average epochs per episode: {average_epochs}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFoqkiKJUQ0v",
        "outputId": "b15b67f6-dcce-448a-fc05-e5a004dc4a63"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 completed.\n",
            "Episode 10 completed.\n",
            "Episode 20 completed.\n",
            "Episode 30 completed.\n",
            "Episode 40 completed.\n",
            "Episode 50 completed.\n",
            "Episode 60 completed.\n",
            "Episode 70 completed.\n",
            "Episode 80 completed.\n",
            "Episode 90 completed.\n",
            "\n",
            "âœ… Evaluation finished!\n",
            "ðŸ“‰ Average penalties per episode: 0.0\n",
            "â±ï¸ Average epochs per episode: 1000.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1e-GcyFcUS5h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}